---
layout:     post
title:      "继续学习pytorch"
subtitle:   "2019/04/15 从新整理代码"
date:       2019-04-15
author:     "WangXiaoDong"
header-img: "https://github.com/Dongzhixiao/PictureCache/blob/master/diaryPic/20190415.jpg?raw=true"
tags:
    - 日记
    - Pytorch
---


```
    最近，使用pytorch重新实现神经网络。
```

### 15号，实现数据预处理

### 16号，实现模型建立

### 17号，实现批处理，顺便向孙师兄学习相关知识，如下：

#### 1. `h_state = h_state.data # 重置隐藏层的状态, 切断和前一次迭代的链接`的含义：
   
一个是张量，一个是里面的数据，数据只是数据本身，不依赖于其它张量，和其它张量没有关系，可以理解成深拷贝

#### 2. h = net.init_hidden(batch_size)# 初始化隐状态的方案，两种：

- 所有样本数据之间有时间上关联性，则每个epoch开始时初始化一次；
- 所有样本数据之间无时间上关联性，则每个小批开始时初始化一次。    

### 18号，完善代码

#### torch.contiguous()使用情景

- 有些tensor并不是占用一整块内存，而是由不同的数据块组成，而tensor的view()操作依赖于内存是整块的，这时只需要执行contiguous()这个函数，把tensor变成在内存中连续分布的形式。 
- 在pytorch的最新版本0.4版本中，增加了torch.reshape(), 这与 numpy.reshape 的功能类似。它大致相当于 tensor.contiguous().view()

#### 隐状态初始化时常用的函数

- torch.zero_() 将tensor填充零
- torch.nn.Module.parameters(): 返回模型参数的迭代器，通常用于传递其给一个优化器

- 19号，测试代码