---
layout:     post
title:      "机器学习（一）"
subtitle:   "线性方程拟合的梯度下降法"
date:       2017-08-05
author:     "WangXiaoDong"
header-img: "https://github.com/Dongzhixiao/PictureCache/blob/master/diaryPic/20170805.jpg?raw=true"
tags:
    - 机器学习
---


### 时间:2017年8月5日 天气:阴:cloud:
-----
#####   Author:冬之晓:angry:
#####   Email: 347916416@qq.com
#####   MyAppearance: ![MyAppearance](https://github.com/Dongzhixiao/PictureCache/raw/master/MyPicture.JPG "我的头像")
----------

# 机器学习

## 引言
定义：一个年代近一点的定义，由 Tom Mitchell 提出，来自卡内基梅隆大学，Tom 定义的机器学习是，一个好的学习问题定义如下，他说，一个程序被认为能从经验 E 中学习，解决任务 T，达到性能度量值P，当且仅当，有了经验 E 后，经过 P 评判，程序在处理 T  时的性能有所提升。经验e  就是程序上万次的自我练习的经验而任务  t  就是下棋。性能度量值  p 呢，就是它在与一些新的对手比赛时，赢得比赛的概率。

目前存在几主要的两种类型被我们称之为监督学习和无监督学习个想法是指我们将教计算机如何去完成任务而在无监督学习中，我们打算让它自己进行。

### 监督学习

eg：可以看出，监督学习指的就是我们给学习算法一个数据集。这个数据集由“正确答案”：可以看出，监督学习指的就是我们给学习算法一个数据集。这个数据集由“正确答案”组成。在房价的例子中，我们给了一系列房子的数据，我们给定数据集中每个样本的正确价格，即它们实际的售价然后运用学习算法，算出更多的正确答案。比如你朋友那个新房子的价格。用术语来讲，这叫做回归问题。
其基本思想是，我们数据集中的每个样本都有相应的“正确答案”。再根据这些样本作出预测，就像房子和肿瘤的例子中做的那样。
我们还介绍了回归问题，即通过回归来推出一个连续的输出，之后我们介绍了分类问题，其目标是推出一组离散的结果。

### 无监督学习

在无监督学习中，我们已知的数据。看上去有点不一样，不同于监督学习的数据的样子，即无监督学习中没有任何的标签或者是有相同的标签或者就是没标签。所以我们已知数据集，却不知如何处理，也未告知每个数据点是什么。别的都不知道，就是一个数据集。你能从数据中找到某种结构吗？针对数据集，无监督学习就能判断出数据有两个不同的聚集簇。
无监督学习算法可能会把这些数据分成两个不同的簇。所以叫做聚类算法
所以这个就是无监督学习，因为我们没有提前告知算法一些信息，比如，这是第一类的人，那些是第二类的人，还有第三类，等等。我们只是说，是的，这是有一堆数据。我不知道数据里面有什么。我不知道谁是什么类型。我甚至不知道人们有哪些不同的类型，这些类型又是什么。但你能自动地找到数据中的结构吗？就是说你要自动地聚类那些个体到各个类，我没法提前知道哪些是哪些。因为我们没有给算法正确答案来回应数据集中的数据，所以这就是无监督学习。

## 单变量线性回归

### 模型表示

我将在整个课程中用小写的 m 来表示训练样本的数目。
以之前的房屋交易问题为例，假使我们回归问题的训练集（Training  Set）如下表所示：
 我们将要用来描述这个回归问题的标记如下:
m  代表训练集中实例的数量
x  代表特征/输入变量
y  代表目标变量/输出变量
(x,y)  代表训练集中的实例
(x(i),y(i) )  代表第  i  个观察实例
h  代表学习算法的解决方案或函数也称为假设（hypothesis）
这就是一个监督学习算法的工作方式，我们可以看到这里有我们的训练集里房屋价格
我们把它喂给我们的学习算法，学习算法的工作了，然后输出一个函数，通常表示为小写 h表示。h  代表  hypothesis(假设)  ，h  表示一个函数，输入是房屋尺寸大小，就像你朋友想出售的房屋，因此  h  根据输入的  x  值来得出  y  值，y 值对应房子的价格  因此，h 是一个从x 到 y 的函数映射。我将选择最初的使用规则  h 代表 hypothesis，因而，要解决房价预测问题，我们实际上是要将训练集“喂”给我们的学习算法，进而学习得到一个假设 h，然后将我们要预测的房屋的尺寸作为输入变量输入给  h，预测出该房屋的交易价格作为输出变量输出为结果。那么，对于我们的房价预测问题，我们该如何表达  h？
一种可能的表达方式为：$$ h_\theta(x)=\theta_0+\theta_1 x $$，  因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题。
 
### 代价函数
我们将定义代价函数的概念，这有助于我们弄清楚如何把最有可能的直线与我们的数据相拟合
在线性回归中我们有一个像这样的训练集，m 代表了训练样本的数量，比如 m = 47。
而我们的假设函数，也就是用来进行预测的函数，是这样的线性函数形式：
$$ h_\theta(x)=\theta_0+\theta_1 x $$
我们的目标便是选择出可以使得建模误差的平方和能够最小的模型参数
即使得代价函数

$$
J(\theta_0,\theta_1)=\frac{1}{2m}
\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2
$$

最小代价函数也被称作平方误差函数，有时也被称为平方误差代价函数。我们之所以要求出误差的平方和，是因为误差平方代价函数，对于大多数问题，特别是回归问题，都是一个合理的选择。还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是解决回归问题最常用的手段了。

### 代价函数的直观理解

$$
\begin{align}
假设: &
h_\theta(x)=\theta_0+\theta_1 x \\
参数：&
\theta_0,\theta_1\\
代价函数：&
J(\theta_0,\theta_1)=\frac{1}{2m}
\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2\\
目标：&
\mathop{minimize}\limits_{\theta_0,\theta_1}J(\theta_0,\theta_1)
\end{align}
$$

### 梯度下降

梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数$$J(\theta_0,\theta_1)$$的最小值。

梯度下降背后的思想是：开始时我们随机选择一个参数的组合（θ0,θ1,...,θn），计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到到
一个局部最小值（local minimum），因为我们并没有尝试完所有的参数组合，所以不能确定
我们得到的局部最小值是否便是全局最小值（global minimum），选择不同的初始参数组合，可能会找到不同的局部最小值。
批量梯度下降（batch gradient descent）算法的公式为：



### 梯度下降的直观理解

梯度下降算法如下:

$$
\theta_j := \theta_j-\alpha\frac{\partial}
{\partial\theta_j}J(\theta)
$$

描述：对 θ 赋值，使得 J(θ)按梯度下降最快方向进行，一直迭代下去，最终得到局部最小值。其中 α 是学习率（learning rate），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大。
让我们来看看如果 α 太小或 α 太大会出现什么情况：
如果 α 太小了，即我的学习速率太小，结果就是只能这样像小宝宝一样一点点地挪动，
去努力接近最低点，这样就需要很多步才能到达最低点，所以如果 α 太小的话，可能会很慢，因为它会一点点挪动，它会需要很多步才能到达全局最低点。如果 α 太大，那么梯度下降法可能会越过最低点，甚至可能无法收敛，下一次迭代又移动了一大步，越过一次，又越过一次，一次次越过最低点，直到你发现实际上离最低点越来越远，所以，如果 α 太大，它会导致无法收敛，甚至发散。现在，我还有一个问题，如果我们预先把 θ1 放在一个局部的最低点，你认为下一步梯度下降法会怎样工作？
假设你将 θ1 初始化在局部最低点，在这儿，它已经在一个局部的最优处或局部最低点。结果是局部最优点的导数将等于零，因为它是那条切线的斜率。这意味着你已经在局部最优点，它使得 θ1 不再改变，也就是新的 θ1 等于原来的 θ1，因此，如果你的参数已经处于局部最低点，那么梯度下降法更新其实什么都没做，它不会改变参数的值。这也解释了为什么即
使学习速率 α 保持不变时，梯度下降也可以收敛到局部最低点。


### 梯度下降的线性回归

梯度下降是很常用的算法，它不仅被用在线性回归上和线性回归模型、平方误差代价函数。梯度下降算法和线性回归算法比较如下：

$$
\begin{align}
&梯度下降算法：\\
&重复直到收敛:\{\\
&\qquad\theta_j := \theta_j-\alpha\frac{\partial}
{\partial\theta_j}J(\theta_0,\theta_1)\\
&\qquad(j=1 和 j=0)\\
&\}\\
&线性回归模型：\\
&\qquad h_\theta(x)=\theta_0+\theta_1 x\\
&\qquad J(\theta_0,\theta_1)=\frac{1}{2m}
\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2
\end{align}
$$

对我们之前的线性回归问题运用梯度下降法，关键在于求出代价函数的导数，即：

$$
\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)=
\frac{\partial}{\partial\theta_j}\frac{1}{2m}
\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2
$$

j=0时：

$$
\frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1)=
\frac{1}{m}
\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})
$$

j=1时：

$$
\frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1)=
\frac{1}{m}
\sum_{i=1}^{m}((h_\theta(x^{(i)})-y^{(i)})\cdot x^{(i)})
$$

则算法改写成：

$$
\begin{align}
&重复\{\\
&\qquad\theta_0:=\theta_0-\alpha
\frac{1}{m}
\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})\\
&\qquad\theta_1:=\theta_1-\alpha
\frac{1}{m}
\sum_{i=1}^{m}((h_\theta(x^{(i)})-y^{(i)})\cdot x^{(i)})\\
\}
\end{align}
$$

这里使用的算法，有时也称为批量梯度下降。实际上，在机器学习中，通常不太会给算法起名字，但这个名字”批量梯度下降”，指的是在梯度下降的每一步中，我们都用到了所有的训练样本，在梯度下降中，在计算微分求导项时，我们需要进行求和运算，所以，在每一个单独的梯度下降中，我们最终都要计算这样一个东西，这个项需要对所有  m  个训练样本求和。因此，批量梯度下降法这个名字说明了我们需要考虑所有这一"批"训练样本，而事实上，有时也有其他类型的梯度下降法，不是这种"批量"型的，不考虑整个的训练集，而是每次只关注训练集中的一些小的子集。但就目前而言，应用刚刚学到的算法，应该已经掌握了批量梯度算法，并且能把它应用到线性回归中了，这就是用于线性回归的梯度下降法。

### 多维特征
首先定义：
$$x^{(i)}_j$$代表第i组训练样本的第j个特征的值
支持多变量的假设h表示为:

$$
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\dotsi+\theta_nx_n
$$

这个公式中有 n+1 个参数和 n 个变量，为了使得公式能够简化一些，引入 x0=1，即：
$$x^{(i)}_0=1$$
此时模型中的参数是一个  n+1  维的向量，任何一个训练实例也都是  n+1  维的向量，特征矩阵
X的维度是m*(n+1)。令：

$$
x=\left[\begin{matrix}x_0\\x_1\\x_2\\\dotsi\\x_n\end{matrix}\right]\in R^{n+1} \qquad
\theta=\left[\begin{matrix}\theta_0\\\theta_1\\\theta_2\\\dotsi\\\theta_n\end{matrix}\right]\in R^{n+1}
$$

则此公式可以简化为：

$$
h_{\theta}(x)=\theta^{\intercal} 	x
$$

#### 多变量梯度下降	
与单变量线性回归类似，在多变量线性回归中，我们也构建一个代价函数，则这个代价函数是所有建模误差的平方和，即：

$$
J(\theta_0，\theta_1，\dotsi，\theta_n)=
\frac{1}{2m}
\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2\\
其中：h_{\theta}(x)=\theta^{\intercal} 	x=\theta_0+\theta_1x_1+\theta_2x_2+\dotsi+\theta_nx_n
$$

我们的目标和单变量线性回归问题中一样，是要找出使得代价函数最小的一系列参数。多变量线性回归的批量梯度下降算法为：

$$
\begin{align}
&重复:\{\\
&\qquad\theta_j := \theta_j-\alpha\frac{\partial}
{\partial\theta_j}J(\theta_0，\theta_1，\dotsi，\theta_n)\\
&\}\\
&即：\\
&重复:\{\\
&\qquad\theta_j := \theta_j-\alpha\frac{\partial}
{\partial\theta_j}\frac{1}{2m}
\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2\\
&\}\\
&求倒数后得到:\\
&重复:\{\\
&\qquad\theta_j := \theta_j-\alpha\frac{1}{m}
\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})\cdot x^{(i)}_j)\\
&\qquad(同时更新\theta_j\quad其中j=0,1,...,n)\\
&\}\\
&当n>=1时：\\
&\qquad\theta_0:= \theta_0-\alpha\frac{1}{m}
\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})\cdot x^{(i)}_0)\\
&\qquad\theta_1:= \theta_1-\alpha\frac{1}{m}
\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})\cdot x^{(i)}_1)\\
&\qquad\theta_2:= \theta_2-\alpha\frac{1}{m}
\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})\cdot x^{(i)}_2)\\
&\qquad\dotsi
\end{align}
$$

我们开始随机选择一系列的参数值，计算所有的预测结果后，再给所有的参数一个新的值，如此循环直到收敛。



