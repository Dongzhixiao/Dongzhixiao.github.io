---
layout:     post
title:      "腾飞的礼物"
subtitle:   "2018/07/29 食物"
date:       2018-07-29
author:     "WangXiaoDong"
header-img: "https://github.com/Dongzhixiao/PictureCache/blob/master/diaryPic/20180729.jpg?raw=true"
tags:
    - 日记
    - 深度学习
    - Keras
---


```
    今天周末，一早腾飞就告诉我他从黑河带来好吃的东西，让我过去拿，然后我就急忙跑过去了，到了之后，
我们一起到他们那里的食堂吃了午饭，我发现他们所食堂午饭的味道非常美味！
```

从这周开始，我也总结keras的使用方法了，因为最近一段时间我通过学习TensorFlow和Keras两大框架，感受
颇深，还是keras好用啊！迫切的希望总结一下，以后可以随时复习！

# keras使用总结
keras是深度学习高层API框架，主要用于神经网络的搭建，我主要通过一个例子从头讲到尾，中间穿插使用的
一些技巧！注意：下面大部分都是根据学习<a target="_blank" href="http://keras-cn.readthedocs.io/en/latest/">keras中文文档</a>后总结得到的。

## 数据输入

keras首先需要有数据的输入，因此输入是多维数组，对于自然语言问题来说，最后一个维度代表每个数据单元
>注意：关于预处理和数据单元的解释在<a target="_blank" href="https://dongzhixiao.github.io/2018/07/21/so-hot/">这篇文章</a>
中有介绍，这里就不再重复解释

独热编码映射到的向量，倒数第二个维度代表一个句子顺序出现的数据单元，倒数第三个维度代表每一批输入
的个数，举个例子，假设输入是好多诗句：

```
當塗當塗見，蕪湖蕪湖見。 
八月十五夜，一似沒柄扇。
...
```

则如果将每个汉字编码，一个单独的汉字“當”编码对应的一个向量[0,0,...,1,...0,0]代表最后一个维度，
这些向量组成的一个句子“當塗當塗見，蕪湖蕪湖見。 ”有13个字符，代表倒数第二个维度。一次输入的批
数，比如一次输入两句话“當塗當塗見，蕪湖蕪湖見。 八月十五夜，一似沒柄扇。”这2个句子代表倒数第
三个维度。

## 数据预处理

keras进行数据预处理有两种方法

### to_categorical

第一种方法是使用功能函数`keras.utils.to_categorical`，这个函数解释如下：

```
to_categorical(y, num_classes=None)
将类别向量(从0到nb_classes的整数向量)映射为二值类别矩阵, 用于应用到以categorical_crossentropy为目标函数的模型中.
参数:
y: 类别向量
num_classes:总共类别数
```

比如输入：

```
from keras.utils import to_categorical
test = to_categorical([1,2,3],5)
print(test)

运行结果：
array([[ 0.,  1.,  0.,  0.,  0.],
       [ 0.,  0.,  1.,  0.,  0.],
       [ 0.,  0.,  0.,  1.,  0.]])
```

可以看出，这个函数实际上是根据输入的每个数据单元编码的位置列表(y)和总共编码的类别数(num_classes)
生成编码后的二维列表。

### Embedding层

第二种方法是使用'keras.layers.embeddings.Embedding'层，这个函数解释如下：

```
keras.layers.embeddings.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)
嵌入层将正整数（下标）转换为具有固定大小的向量，如[[4],[20]]->[[0.25,0.1],[0.6,-0.2]]
Embedding层只能作为模型的第一层
参数
input_dim：大或等于0的整数，字典长度，即输入数据最大下标+1
output_dim：大于0的整数，代表全连接嵌入的维度
embeddings_initializer: 嵌入矩阵的初始化方法，为预定义初始化方法名的字符串，或用于初始化权重的初始化器。参考initializers
embeddings_regularizer: 嵌入矩阵的正则项，为Regularizer对象
embeddings_constraint: 嵌入矩阵的约束项，为Constraints对象
mask_zero：布尔值，确定是否将输入中的‘0’看作是应该被忽略的‘填充’（padding）值，该参数在使用递归层处理变长输入时有用。设置为True的话，模型中后续的层必须都支持masking，否则会抛出异常。如果该值为True，则下标0在字典中不可用，input_dim应设置为|vocabulary| + 1。
input_length：当输入序列的长度固定时，该值为其长度。如果要在该层后接Flatten层，然后接Dense层，则必须指定该参数，否则Dense层的输出维度无法自动推断。
输入shape：形如（samples，sequence_length）的2D张量
输出shape：形如(samples, sequence_length, output_dim)的3D张量
```

比如输入：

```
Embedding(input_dim=1000, output_dim=100,input_length=line_length)
```

代表的就是输入的维度是1000个维度，然后输出就编码成100个维度的向量了，这个层首先使得输入的维度
增加了一个维度，其次还满足的降维的作用，也就是说如果需要使用这个层的话，一开始预处理就不需要
使用`to_categorical`函数预测理了！


## 显示层的图像

这里跳过网络的搭建，因为搭建的两种方式（）已经在说明文档中写的很清楚了，而且通读了搭建方法，
我发现最好的学习方法是自己构建网络，然后时候绘图绘制出网络结构，通过结构显示出每一层的输入
输出的维度，进行对比来学习。

可视化<a target="_blank" href="http://keras-cn.readthedocs.io/en/latest/other/visualization/">这里</a>已经有详细解释了，下面主要说明安装可视化需要的工具

### 安装绘图工具包

为了绘制出网络结构，需要安装工具`graphviz`和工具`pydot-ng`。注意顺序，先安装`graphviz`
注意，安装`graphviz`可以直接使用pip安装(pip install graphviz)，或者直接去官网下载，比如
windows版本就在<a target="_blank" href="https://graphviz.gitlab.io/_pages/Download/Download_windows.html">这里</a>下载，
之后解压缩后一定要把里面的bin加入的系统环境变量PATH里面这样使得其中的`dot`命令可以让系统找到！

然后别急着安装`pydot-ng`，先使用下面代码测试：

```

```

如果成功的生成了绘图结果，则继续往后面做！

然后使用命令(pip install pydot-ng)安装`pydot-ng`即可。

之后在需要绘图的地方使用代码

## 损失函数的显示loss



## 性能评估Metrices

性能评估函数在<a target="_blank" href="http://keras-cn.readthedocs.io/en/latest/other/metrics/">这里</a>
有详细的解释，但是看完后我还会有疑问，就是能够使用的字符串一共有多少种？

为什么需要精度的时候传入字符串“accruacy”和“acc”都行？
如果需要解答的话我们要查看`Keras`的源代码，找到


## 常用的回调函数Callback

### TensorBoard

`TensorBoard`是非常用的回调函数，因为这个函数可以将所需要的数据保存成能够用于`TensorBoard`使用
显示的方法。

使用方法非常简单，如下：
```
tensor_board = keras.callbacks.TensorBoard()  #数据输出到TensorBoard
```

### 设置检查点保存

通常在使用神经网络训练模型的时候，没进行一批都会更新一次所有的权重参数，

```
#下面设置模型保存点,一步检测一次，只保存最好的模型
    check_point = keras.callbacks.ModelCheckpoint(
            filepath='logs\weights-{epoch:03d}-{val_acc:.4f}.hdf5',
            monitor='val_acc',
            period=1,
            save_best_only=True) 
```

### attention机制

为了将文字之间的内容关联的更加紧密，我们可以实现attention机制。


```

```

### 自定义层

如果我们打开这里，就可以看到如何实现一个自定义层，我们可以发现

