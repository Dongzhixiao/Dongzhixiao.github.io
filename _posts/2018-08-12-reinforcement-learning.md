---
layout:     post
title:      "让我们用奖励机制鼓励程序吧"
subtitle:   "2018/08/12 训练"
date:       2018-08-12
author:     "WangXiaoDong"
header-img: "https://github.com/Dongzhixiao/PictureCache/blob/master/diaryPic/20180812.jpg?raw=true"
tags:
    - 日记
    - 强化学习
---


```
    今天周日，还是不想出门，果然我就适合过宅在家的生活，顺便在屋子里研究一下增强学习，发现原来增强
学习非常有趣，跟马尔科夫模型有点像，都是研究一堆状态直接转换，只不过强化学习在每个状态对应的动作上
都有一个打分机制，即所谓的惩罚和奖励，通过一个算法进行选择，另一个算法进行更新状态动作分数表格，最终
就得到了强化学习算法，非常有趣，晚上和妈妈爸爸在外面散步，顺便给他们讲讲今天学到的强化学习的内容，
不过显然他们没啥兴趣，只能我自得其乐了……
```

## 强化学习之Q-learning

### 基本概念

强化学习中最核心的几个概念为：
- 智能体（ Agent ）。
- 环境（ Environment ）。
- 动作（ Action ）。
- 奖励（ Reward ） 。

概念：智能体存在于环境中，并会在环境中做出一些动作。这些动作会使得智
能体获得一些奖励。奖励可能有正，也可能有负。强化学习的目标是学习一
个策略，使得智能体可以在合适的时候做出合适的动作，以获得最大的奖励。

此外，还有一个重要的概念：状态。顾名思义，“状态”描述了智能体
和环境的状况，它和环境以及智能体都有关。智能体一般以当前的状态作为
决策依据，做出决策后， 智能体的行为又会引起状态的改变。

### 例子

作为示例，可以考虑这样一个简单的“走迷宫”的例子：

```
.........
.A      .
.     o .
.       .
.........
```

这是一个非常简单的“迷宫”，“A”表示智能体。它一共有4个动作：
向上走、向下走、向左走、向右走。实心点号“.”表示迷宫的边缘，智能
体在行走时不能逾越这个边缘。而“o”表示一个“ 宝藏”，当智能体走到
宝藏的位置时，将会自动获得值为100的奖励。
在这个例子中:
- 智能体是图中的“A”，它可以在不超过迷宫边缘的范围内自由行走。
- 环境是指整个迷宫。
- 动作是指上、下、左、右四个可以采取的行为。
- 奖励的含义为：在智能体走到空白位置时，奖励为0 ， 而在走到宝藏位置时，奖励为100 。
- 状态的含义：可以用智能体在迷宫中的位置（第几行第几列）来表示状
态。智能体必须根据状态（即它目前所在的位置）做出决策，以获得最大的奖励。

### 核心Q函数

Q_learning 算法的核心是$$Q(s, a)$$函数，其中$$s$$表示状态，$$a$$表示行为。$$Q$$
函数可以被看作一个“表格”，每一行代表一个状态，每一列代表一个有为，见下表：

|-|-|-|-|-| 
||行为$$a1$$ | 行为$$a2$$ | 行为$$a3$$ | ... |
|状态$$s1$$ | $$Q(s1, a1)$$ | $$Q(s1, a2)$$ | $$Q(s1, a3)$$ | ...|
|状态$$s2$$ | $$Q(s2, a1)$$ | $$Q(s2, a2)$$ | $$Q(s2, a3)$$ | ... |
|状态$$s3$$ | $$Q(s3, a1)$$ | $$Q(s3, a2)$$ | $$Q(s3, a3)$$ | ... |
| ...   | ...       | ...       | ...       | ... |    

$$Q(s ，a)$$的值是“在$$s$$状态执行了$$a$$行为后的期望奖励数值”。只要得到了
正确的$$Q$$函数，就可以在每个状态做出合适的决策了。例如，在问题中，设$$s$$
为某个位置$$s1$$，如何决定智能体下一步应该往哪里走呢？由于$$a$$的取值为0，
1，2，3 ，只需要考虑$$Q(s1,0)，Q(s1,l)，Q(s1,2)，Q(s1,3)$$四个值，并挑选
其中最大的并执行相应的动作即可。

### Q函数的学习策略

更新Q值，对应的公式为

$$Q(s,a)\longleftarrow(1-\alpha)Q(s,a)+\alpha[R+\gamma max_a Q(s',a)]$$

$$Q(state, action)$$表示在状态state下执行action后**期望**得到的
奖励。实际上，执行action 后状态自state变成了new_state，并且得到了奖
励reward。Q_learning算法用$$reward+\gamma*Q[new\_state, : ].max()$$来近
似期望得到的奖励。其中，reward 为执行action 后本来可以得到的奖励，而
$$Q[new\_state, :].max()$$是在新状态new_state 下可以得到的最大期望奖励。由
于$$Q[new\_state, :].max()$$是下一个时间点的值，因此还要乘以一个衰减系数
$$\gamma$$。最后，一般不直接用$$reward+\gamma*Q[new\_state, : ].max()$$更
新$$Q(state,action)$$，而是设置一个学习率$$\alpha$$，用$$(1-\alpha)Q(s,a)+\alpha[R+\gamma max_a Q(s',a)]$$
来更新$$Q(state,action)$$，这样的话更新比较平缓，防止模型过早收敛到局部极小值。这是Q
Learning更新Q函数的方法。

### $$\epsilon-greedy$$策略

在Q Learning 的更新过程中，每一步都要根据当前的state 以及Q 函数
确定一个合适的行动action。这里高一个如何平衡“经验”和“探索”的问
题。如果完全肢照经验行动，即每次都在Q(state, :) 中选择对应值最大的
action ， 那么很高可能一直局限在已高经验中，难以发现更具价值的新的行
为。但如果智能体只专注于探索新的行为，即完全随机地行动，又可能因为
大多数行动都没高价值，导致学习Q函数的速度很慢。

一种比较简单的平衡“经验”和“探索”的方法是采用$$\epsilon-greedy$$策略选
择合适的行动。事先设置一个较小的$$\epsilon$$值（如$$\epsilon=0.1$$），
智能体有$$1-\epsilon$$的慨率根据学习到的Q函数（己有经验）行动，剩下$$\epsilon$$ 
的概率智能体会随机行动，用于探索新的经验。

例如，E = 0.1 时，在90% 的情况下，智能体直接选择使
得Q(state, action)最大的action ，剩下10% 的情况，随机选择一个action 。





