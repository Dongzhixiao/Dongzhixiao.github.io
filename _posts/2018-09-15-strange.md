---
layout:     post
title:      "总结attention机制"
subtitle:   "2018/09/15 绘图"
date:       2018-09-15
author:     "WangXiaoDong"
header-img: "https://github.com/Dongzhixiao/PictureCache/blob/master/diaryPic/20180915.jpg?raw=true"
tags:
    - 日记
    - keras
---


```
    今天是周六，早晨和师兄去健身，主要原因是昨天晚上师兄有事情，因此就临时改到了今天早晨。之后我就
在屋子里洗了一下衣服。
```

# 古诗生成模型(续)

## 背景



本文后续章节安排如下：

- 介绍 
- 详细说明attention机制所使用的方法以及如何将输出的attention结果绘制出正确的图像
- 最后总结全文

## 介绍

之前在<a target='_blank' href='https://dongzhixiao.github.io/2018/08/11/poetry/'>这篇文章</a>里面已经介绍了如何使用神经网络
进行生成模型的使用，在那篇文章里面我使用的attention机制并没好好介绍，并且如何使用之绘图也没有详细
说明，最近仔细研究了一下，得出了结论。

## 方法

### attention机制原理以及核心代码

为了实现attention，首先需要将时间步进行全连接网络训练，这是因为attention机制的核心就是直接作用于时间步上，
尝试直接通过时间的连接找到其中的重要性差异。对于LSTM，我参考了网上比较<a href='' target='_blank'>简单的实现方法</a>


### attention机制绘制出对应图片的方法

为了将结果绘制出来，我们需要知道权重具体在哪一层。通过因为具体的数据得到后才能根据数据绘图。
keras里面有方法可以得到需要的输出层，不过一定要使用贯序模型。

## 结论

本文通过介绍LSTM实现的







