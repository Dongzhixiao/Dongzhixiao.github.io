---
layout:     post
title:      "周末的休息"
subtitle:   "2018/09/22 休息"
date:       2018-09-22
author:     "WangXiaoDong"
header-img: "https://github.com/Dongzhixiao/PictureCache/blob/master/diaryPic/20180922.jpg?raw=true"
tags:
    - 日记
    - HMM
---


```
    今天周六，而且也是属于中秋节的假期，因此我就好好在宿舍休息了一下，然后顺便总结了一下上周学习到的
隐马尔科夫模型。
    晚上，去健身房锻炼，本来以为周六没有什么人，结果发现人非常的多！洗澡的时候还等了好久，这说明有健
身意识的人真的非常多。
    锻炼完之后，我发现网上买的手机电池到了，因此回到宿舍时顺便吧电池拿了上来。然后就开始尝试安装小米5
的电池，我本来以为安一个手机电池而已，应该很简单。但是事实上操作起来却不是那么简单的，我首先安装网上
的教程吧后盖通过一个吸盘去掉，然后将主板保护外壳上面的六个螺丝依次扭下来，接着就把和主板相连接位置的
电池扣去掉。下来网上的教程写的不清楚，说把双面胶去掉，我就随便去掉，发现吧双面胶扭断了，然后我才意识
到电池怎么都去不下来。这时候我赶快搜相关视频教程，从教程中发现双面胶扭的方向不是直着向外拉，而是要向
侧方向拉开，但是这时候已经晚了。此时我非常着急，就拿着螺丝刀硬要把电池扭开，结果令我害怕的是电池突然
冒起了火星！
    这时候我终于后悔了，早知道应该找专门人员帮帮忙！现在为了防止电池爆炸，我只能将电池放到阳台冷却。
这时候已经非常晚了，我只能先休息，等到明天再想办法！
```

进行总结一下上周周内学习到的HMM模型。

## 隐马尔科夫模型HMM

### 介绍

机器学习的最重要任务是通过得到的训练样本来对未知变量进行估计和推断。概率模型提供了一种描述框架，即如何
通过可观测变量推断出未知变量的条件分布，计算的方式可以分为两类：

- 判别式模型（Discriminative Model）是直接对条件概率$$p(y\mid x;\theta)$$建模。常见的判别式模型有 线性回归模型、线性判别分析、支持向量机SVM、神经网络等。
- 生成式模型（Generative Model）则会对$$x$$和$$y$$的联合分布$$p(x,y)$$建模，然后通过贝叶斯公式来求得$$p(y_i\mid x)$$，然后选取使得$$p(y_i\mid x)$$最大的$$y_i$$，即：
$$arg max_y p(y\mid x)=arg max_y \frac{p(x\mid y)p(y)}{p(x)}=arg max_y p(x\mid y)p(y)$$常见的生成式模型有 隐马尔可夫模型HMM、朴素贝叶斯模型、高斯混合模型GMM、LDA等。


由此可见，隐马尔可夫模型HMM是属于典型的生成式模型，其有两个特征：

- 我们的问题是基于序列的，比如时间序列，或者状态序列
- 我们的问题中有两类数据，一类序列数据是可以观测到的，即观测序列；而另一类数据是不能观察到的，即隐藏状态序列，简称状态序列

有了这两个特征，那么这个问题一般可以用HMM模型来尝试解决。这样的问题在实际生活中是很多的。比如：我现在在打字写博客，我在键盘
上敲出来的一系列字符就是观测序列，而我实际想写的一段话就是隐藏序列，输入法的任务就是从敲入的一系列字符尽可能的猜测我要写的
一段话，并把最可能的词语放在最前面让我选择，这就可以看做一个HMM模型了。再举一个，我在和你说话，我发出的一串连续的声音就是观
测序列，而我实际要表达的一段话就是状态序列，你大脑的任务，就是从这一串连续的声音中判断出我最可能要表达的话的内容。

### HMM模型的定义

对于HMM模型，首先我们假设Q是所有可能的隐藏状态的集合，V是所有可能的观测状态的集合，即：

$$Q = \{q_1,q_2,...,q_N\}, \; V =\{v_1,v_2,...v_M\}$$

其中，$$N$$是可能的隐藏状态数，$$M$$是所有的可能的观察状态数。

对于一个长度为$$T$$的序列，I对应的状态序列, $$O$$是对应的观察序列，即：
$$I = \{i_1,i_2,...,i_T\}, \; O =\{o_1,o_2,...o_T\}$$

其中，任意一个隐藏状态$$i_t\in Q$$,任意一个观察状态$$o_t\in V$$

HMM模型做了两个很重要的假设如下：

- 1） 齐次马尔科夫链假设。即任意时刻的隐藏状态只依赖于它前一个隐藏状态，当然这样假设有点极端，
因为很多时候我们的某一个隐藏状态不仅仅只依赖于前一个隐藏状态，可能是前两个或者是前三个。
但是这样假设的好处就是模型简单，便于求解。如果在时刻t
的隐藏状态是$$i_t=q_i$$,在时刻$$t+1$$的隐藏状态是$$i_t+1=qj$$, 则从时刻t到时刻$$t+1$$的HMM状态转移概率$$a_ij$$可以表示为：

$$aij=P(it+1=qj|it=qi)$$

这样$$a_ij$$可以组成马尔科夫链的状态转移矩阵
$$A=\Big [a_{ij}\Big ]_{N \times N}$$

- 2） 观测独立性假设。即任意时刻的观察状态只仅仅依赖于当前时刻的隐藏状态，这也是一个为了简化模型的假设。如果在时刻t
的隐藏状态是$$i_t=q_j$$, 而对应的观察状态为$$o_t=v_k$$, 则该时刻观察状态$$v_k$$在隐藏状态$$q_j$$下生成的概率为$$b_j(k)$$,满足：
$$B = \Big [b_j(k) \Big ]_{N \times M}$$

这样$$b_j(k)$$可以组成观测状态生成的概率矩阵B:
$$B = \Big [b_j(k) \Big ]_{N \times M}$$

除此之外，我们需要一组在时刻$$t=1$$的隐藏状态概率分布$$\Pi$$:
$$\Pi = \Big [ \pi(i)\Big ]_N \; 其中 \;\pi(i) = P(i_1 = q_i)$$其中$$\Pi = \Big [ \pi(i)\Big ]_N \; 其中 \;\pi(i) = P(i_1 = q_i)$$
一个HMM模型，可以由隐藏状态初始概率分布$$Π$$, 状态转移概率矩阵A和观测状态概率矩阵B决定。Π,A决定状态序列，B决定观测序列。
因此，HMM模型可以由一个三元组$$λ$$表示如下：
$$\lambda = (A, B, \Pi)$$


### 三个经典的问题

HMM模型一共有三个经典的问题需要解决：

- 1） 评估观察序列概率。即给定模型$$\lambda = (A, B, \Pi)$$和观测序列$$O =\{o_1,o_2,...o_T\}$$，计算在模型λ下观测序列O出现的概率$$P(O\mid\lambda)$$

。这个问题的求解需要用到前向后向算法，这个问题是HMM模型三个问题中最简单的。

- 2）模型参数学习问题。即给定观测序列$$O =\{o_1,o_2,...o_T\}$$，估计模型$$\lambda = (A, B, \Pi)$$的参数，使该模型下观测序列的条件概率$$P(O\mid\lambda)$$
最大。这个问题的求解需要用到基于EM算法的鲍姆-韦尔奇算法， 这个问题是HMM模型三个问题中最复杂的。

- 3）预测问题，也称为解码问题。即给定模型$$\lambda = (A, B, \Pi)$$
和观测序列$$O =\{o_1,o_2,...o_T\}$$，求给定观测序列条件下，最可能出现的对应的状态序列，这个问题的求解需要用到基于动态规划的维特比算法，
这个问题是HMM模型三个问题中复杂度居中的算法。














