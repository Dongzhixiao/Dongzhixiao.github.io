---
layout:     post
title:      "机器学习（二）"
subtitle:   "线性方程拟合的正规方程法*（推荐网址）"
date:       2017-08-21
author:     "WangXiaoDong"
header-img: "img/20170805.jpg"
tags:
    - 机器学习
---


### 时间:2017年8月21日 天气:阴:cloud:
-----
#####   Author:冬之晓:angry:
#####   Email: 347916416@qq.com
#####   MyAppearance: ![MyAppearance](https://github.com/Dongzhixiao/PictureCache/raw/master/MyPicture.JPG "我的头像")
----------

到目前为止，我们都在使用梯度下降算法，但是对于某些线性回归问题，正规方程方法是更好的解决方案。如：
$$
J(\theta)=a\theta^2+b\theta+c
$$
正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：
$$
\frac{\partial}
{\partial\theta_j}J(\theta_j)=0
$$

假设我们的训练集特征矩阵为 X（包含了 x0=1）并且我们的训练集结果为向量 y，则利用正规方程解出向量：
$$
\theta = (X^HX)^{-1}X^Hy
$$
以下表示数据为例：
|$$x_0$$|尺寸(英尺)$$x_1$$|房间数$$x_2$$|楼层数$$x_3$$|使用时间(年)$$x_4$$|价格(1000dollar)$$y$$
|-|-|-|-|
|1|2104|5|1|45|460|
|1|1416|3|2|40|232|
|1|1534|3|2|30|315|
|1|852|2|1|36|178|
$$
\left[\begin{matrix}1&2104&5&1&45\\1&1416&3&2&40\\1&1534&3&2&30\\1&852&2&1&36\\\end{matrix}\right]
$$
使用python可以这样计算：
```
import numpy as np
X = np.array([[1,2104,5,1,45],[1,1416,3,2,40],[1,1534,3,2,30],[1,852,2,1,36]])
y = np.array([[460],[232],[315],[178]])
result = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)
```
>注：对于那些不可逆的矩阵（通常是因为特征之间不独立，如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征，也有可能是特征数量大于训练集的数量），正规方程方法是不能用的。python中常用于机器学习算法的库的介绍网站在：http://www.usyiyi.cn/translate/scipy_lecture_notes/index.html选取机器学习的方法和特征可参考：http://blog.jobbole.com/85680/

梯度下降与正规方程的比较：
|梯度下降|正规方程|
|-|-|
|需要选择学习率 α|不需要|
|需要多次迭代|一次运算得出|
|当特征数量 n 大时也能较好适用|如果特征数量 n 较大则运算代价大，因为矩阵逆的计算时间复杂度为O(n^3)，通常来说当n小于10000时还是可以接受的|
|适用于各种类型的模型|只适用于线性模型，不适合逻辑回归模型等其他模型|

总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数  θ  的替代方法。具体地说，只要特征变量数量小于一万，我通常使用标准方程法，而不使用梯度下降法。


随着我们要讲的学习算法越来越复杂，例如，当我们讲到分类算法，像逻辑回归算法，我们会看到， 实际上对于那些算法，并不能使用标准方程法。对于那些更复杂的学习算法，我们将不得不仍然使用梯度下降法。因此，梯度下降法是一个非常有用的算法，可以用在有
大量特征变量的线性回归问题。或者我们以后在课程中，会讲到的一些其他的算法，因为标准方程法不适合或者不能用在它们上。但对于这个特定的线性回归模型，标准方程法是一个比梯度下降法更快的替代算法。所以，根据具体的问题，以及你的特征变量的数量，这两种算法都是值得学习的。
