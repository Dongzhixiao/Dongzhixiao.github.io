---
layout:     post
title:      "在宿舍休息"
subtitle:   "2018/08/19 休息"
date:       2018-08-19
author:     "WangXiaoDong"
header-img: "https://github.com/Dongzhixiao/PictureCache/blob/master/diaryPic/20180819.jpg?raw=true"
tags:
    - 日记
    - 强化学习
---


```
    今天已经是回到宿舍的第二天了，我专门提前回来一天就是为了能够好好休息一下，为明天的工作做准备。
```

## 强化学习之DQN

### 原因

在普通的Q-learning中，当状态和动作空间是离散且维数不高时可使用Q-Table储存每个状态动作对的Q值，而当状态和动作空间是高维连续时，使用Q-Table不现实。

通常做法是把Q-Table的更新问题变成一个函数拟合问题，相近的状态得到相近的输出动作。如下式，通过更新参数 θθ 使Q函数逼近最优Q值 

$$
Q(s,a;\theta)\approx Q'(s,a)
$$

而深度神经网络可以自动提取复杂特征，因此，面对高维且连续的状态使用深度神经网络最合适不过了。

### 内容

DQN 算法用一个**深度卷积神经网络**来表示Q 函数。它的输入是状态s,
输出是每个动作对应的Q函数值。假设一共有4种动作，用0，1，2，3来
表示，那么神经网络的输出是Q(s, 0) , Q(s, 1) , Q(s , 2), Q(s, 3) 。这个神经网络
叫Deep Q Network 。    

有了Deep Q Network 后，剩下的问题变成了：如何训练这个神经网络？
在DQN 算法中，原作者提出了一种经验回放机制（ experience replay
mechanism ），用来产生神经网络的训练样本。

智能体首先会尝试玩游戏，在此过程中累积经验，形成一个“经验池”。
用$$D = {e_1 ,e_2, ... ,e_N}$$来表示经验池，其中$$e_t = (s_t ，a_t，r_t ， s_{t+1})$$。
$$s_t$$是在t时刻的状态(即前文所述的连续4帧画面，一个84×84x4的张量)，$$a_t$$为在t时刻采
取的行动，$$r_t$$为获得的奖励（得分），$$s_{t+1}$$是下一个时刻的状态，即新观察到
的连续4帧画面。
每步训练，都在经验池中选取batch size 个$$e_t$$，作为训练样本。每个样本
对应的训练目标（即Deep Q Network 的输出）和Q Leaming 一样，都是
$$r_t+\gammarmax_a' maxQ(s_{t+1},a')$$。
训练若干步后，得到了一个新的Q Network 。利用它再来玩游戏，这又
会得到一系列$$e_t$$，将得到的$$e_t$$加入经验池中用于训练，依此类推。
DQN 算法对应的伪代码为:

```
for episode=1,M do
    #初始化状态$$s_1$$
    for t=1, T do
    用$$\epsilon-greedy$$方法根据当前状态$$s_t$$选择动作$$a_t$$
    执行$$a_t$$，获得奖励$$r_t$$，并观察到新状态$$s_t+1$$
    将
    在D中随机选取
    设定$$y_j$$:
        若
```



